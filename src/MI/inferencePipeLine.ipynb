{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQbRCMu71_z9"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qq7LBICM1u8n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import FastICA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import iirnotch, filtfilt, butter , coherence, csd, welch, hilbert\n",
        "import scipy.signal as signal\n",
        "from scipy.stats import kurtosis , skew"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "################################################\n",
        "# Replace with your path of data to be infered #\n",
        "################################################\n",
        "Path_of_the_data_set = r\"C:\\Users\\Badr\\Desktop\\mtcaic3\\MI\\test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SoSJWk1h1w90"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------\n",
        "# Independent Component Analysis (ICA) helper\n",
        "# ---------------------------------------------------------------------------\n",
        "def apply_ica(data):\n",
        "    data = np.asarray(data)                           # ensure ndarray\n",
        "    if data.ndim == 1:                                # reshape 1‑D → 2‑D\n",
        "        data = data[:, None]\n",
        "\n",
        "    ica = FastICA(                                    # ICA model\n",
        "        n_components=data.shape[1],\n",
        "        random_state=42,\n",
        "        max_iter=2000,\n",
        "        tol=1e-5\n",
        "    )\n",
        "\n",
        "    S = ica.fit_transform(data)                       # sources\n",
        "    k = np.abs(kurtosis(S, axis=0, fisher=False))     # kurtosis per source\n",
        "    S[:, k > 5] = 0                                   # suppress artifactual comps\n",
        "    cleaned = ica.inverse_transform(S)                # back‑project\n",
        "    return cleaned.squeeze()                          # drop singleton dims\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# End‑to‑end preprocessing + feature extraction\n",
        "# ---------------------------------------------------------------------------\n",
        "def preprocessing_and_loading(\n",
        "    subjects_of_train,\n",
        "    path_of_MI_diret_train,\n",
        "    output_csv=\"MI_features.csv\"\n",
        "):\n",
        "    \"\"\"Pipeline: load raw MI data → preprocess → extract features → CSV.\n",
        "\n",
        "    Each row in the resulting CSV corresponds to one trial.\n",
        "    \"\"\"\n",
        "    # ----------------------------- constants --------------------------------\n",
        "    fs = 250                          # sampling frequency (Hz)\n",
        "    baseline_end = fs                 # 1‑s baseline (first 250 samples)\n",
        "    fbcsp_bands = [                   # FBCSP sub‑bands (Hz)\n",
        "        (8, 12), (12, 16), (16, 20),\n",
        "        (20, 24), (24, 28), (28, 32)\n",
        "    ]\n",
        "\n",
        "    # -------------------- helper: band‑power (FFT) --------------------------\n",
        "    def bandpower(sig, f_lo, f_hi):\n",
        "        \"\"\"Relative power within [f_lo, f_hi] using periodogram.\"\"\"\n",
        "        freqs = np.fft.rfftfreq(len(sig), 1 / fs)\n",
        "        pxx = np.abs(np.fft.rfft(sig)) ** 2\n",
        "        band_mask = (freqs >= f_lo) & (freqs <= f_hi)\n",
        "        return np.sum(pxx[band_mask]) / len(sig)\n",
        "\n",
        "    # ----------------------------- main loop --------------------------------\n",
        "    rows, X_all = [], []\n",
        "\n",
        "    for sbj in subjects_of_train:\n",
        "        # Notch + band‑pass filters\n",
        "        f0, Q = 50, 30                               # notch (50 Hz line noise)\n",
        "        b_notch, a_notch = iirnotch(f0, Q, fs)\n",
        "        b_bp, a_bp = butter(4, [8 / (fs / 2), 30 / (fs / 2)], btype=\"band\")\n",
        "\n",
        "        sbj_path = os.path.join(path_of_MI_diret_train, sbj)\n",
        "\n",
        "        for sess in os.listdir(sbj_path):\n",
        "            # Assumes a single CSV per session directory\n",
        "            csv_path = os.path.join(sbj_path, sess,\n",
        "                                    os.listdir(os.path.join(sbj_path, sess))[0])\n",
        "            data_df = pd.read_csv(csv_path)\n",
        "\n",
        "            for tr in range(1, 11):                  # 10 trials per session\n",
        "                # ------------------------------------------------------------------\n",
        "                # 1) Segment & ICA cleaning\n",
        "                # ------------------------------------------------------------------\n",
        "                seg = data_df.iloc[2250 * (tr - 1):2250 * tr, 1:9].to_numpy()\n",
        "                seg = apply_ica(seg)\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "                # 2) Filtering (notch → band‑pass)\n",
        "                # ------------------------------------------------------------------\n",
        "                c3 = filtfilt(b_notch, a_notch, seg[:, 1] - seg[:, 1].mean())\n",
        "                cz = filtfilt(b_notch, a_notch, seg[:, 2] - seg[:, 2].mean())\n",
        "                c4 = filtfilt(b_notch, a_notch, seg[:, 3] - seg[:, 3].mean())\n",
        "\n",
        "                for ch in range(seg.shape[1]):                       # band‑pass all\n",
        "                    seg[:, ch] = filtfilt(b_bp, a_bp, seg[:, ch])\n",
        "\n",
        "                c3 = filtfilt(b_bp, a_bp, c3)\n",
        "                cz = filtfilt(b_bp, a_bp, cz)\n",
        "                c4 = filtfilt(b_bp, a_bp, c4)\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "                # 3) Baseline correction (ERD/ERS)\n",
        "                # ------------------------------------------------------------------\n",
        "                c3 -= c3[:baseline_end].mean()\n",
        "                cz -= cz[:baseline_end].mean()\n",
        "                c4 -= c4[:baseline_end].mean()\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "                # 4) Time‑domain features\n",
        "                # ------------------------------------------------------------------\n",
        "                row = {\n",
        "                    \"subject_id\": sbj,\n",
        "                    \"session_id\": sess,\n",
        "                    \"trial_id\": tr,\n",
        "                    # basic stats (C3)\n",
        "                    \"min_val_C3\":   float(c3.min()),\n",
        "                    \"max_val_C3\":   float(c3.max()),\n",
        "                    \"mean_val_C3\":  float(c3.mean()),\n",
        "                    \"energy_C3\":    float(np.sum(c3 ** 2)),\n",
        "                    # basic stats (C4)\n",
        "                    \"min_val_C4\":   float(c4.min()),\n",
        "                    \"max_val_C4\":   float(c4.max()),\n",
        "                    \"mean_val_C4\":  float(c4.mean()),\n",
        "                    \"energy_C4\":    float(np.sum(c4 ** 2))\n",
        "                }\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "                # 5) ERD/ERS (8–30 Hz band power)\n",
        "                # ------------------------------------------------------------------\n",
        "                base_c3 = bandpower(c3[:baseline_end], 8, 30)\n",
        "                task_c3 = bandpower(c3[baseline_end:], 8, 30)\n",
        "                base_c4 = bandpower(c4[:baseline_end], 8, 30)\n",
        "                task_c4 = bandpower(c4[baseline_end:], 8, 30)\n",
        "\n",
        "                row.update({\n",
        "                    \"alpha_C3\":  float(bandpower(c3, 8, 12)),\n",
        "                    \"alpha_C4\":  float(bandpower(c4, 8, 12)),\n",
        "                    \"erd_C3_pct\": 10 * np.log10(task_c3 / base_c3) if base_c3 else 0.0,\n",
        "                    \"erd_C4_pct\": 10 * np.log10(task_c4 / base_c4) if base_c4 else 0.0\n",
        "                })\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "                # 6) Connectivity: coherence / imaginary coherence / PLV\n",
        "                # ------------------------------------------------------------------\n",
        "                f_coh, coh = signal.coherence(c3, c4, fs=fs, nperseg=fs * 2)\n",
        "                mu_band = (f_coh >= 8) & (f_coh <= 30)\n",
        "                coh_mu = float(coh[mu_band].mean()) if mu_band.any() else 0.0\n",
        "\n",
        "                f_csd, Pxy = signal.csd(c3, c4, fs=fs, nperseg=fs * 2)\n",
        "                f_pxx, Pxx = signal.welch(c3, fs=fs, nperseg=fs * 2)\n",
        "                f_pyy, Pyy = signal.welch(c4, fs=fs, nperseg=fs * 2)\n",
        "                imcoh = np.imag(Pxy) / np.sqrt(Pxx * Pyy)\n",
        "                imcoh_mu = float(imcoh[(f_csd >= 8) & (f_csd <= 30)].mean())\n",
        "\n",
        "                phi = np.angle(signal.hilbert(c3)) - np.angle(signal.hilbert(c4))\n",
        "                plv_val = float(np.abs(np.exp(1j * phi).mean()))\n",
        "\n",
        "                row.update({\n",
        "                    \"coh_C3C4_mu\":   coh_mu,\n",
        "                    \"imcoh_C3C4_mu\": imcoh_mu,\n",
        "                    \"plv_C3C4\":      plv_val\n",
        "                })\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "                # 7) Higher‑order stats\n",
        "                # ------------------------------------------------------------------\n",
        "                row.update({\n",
        "                    \"range_C3\": float(np.ptp(c3)),\n",
        "                    \"std_C3\":   float(c3.std()),\n",
        "                    \"skew_C3\":  float(skew(c3)),\n",
        "                    \"kurt_C3\":  float(kurtosis(c3)),\n",
        "                    \"range_C4\": float(np.ptp(c4)),\n",
        "                    \"std_C4\":   float(c4.std()),\n",
        "                    \"skew_C4\":  float(skew(c4)),\n",
        "                    \"kurt_C4\":  float(kurtosis(c4))\n",
        "                })\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "                # 8) Frequency‑domain features (whole 0–40 Hz spectrum)\n",
        "                # ------------------------------------------------------------------\n",
        "                freqs = np.fft.rfftfreq(len(c3), 1 / fs)\n",
        "                mask40 = freqs <= 40\n",
        "                power_c3 = np.abs(np.fft.rfft(c3)) ** 2\n",
        "                power_c4 = np.abs(np.fft.rfft(c4)) ** 2\n",
        "\n",
        "                mean_f_c3 = np.sum(freqs * power_c3) / np.sum(power_c3)\n",
        "                mean_f_c4 = np.sum(freqs * power_c4) / np.sum(power_c4)\n",
        "                bw_c3 = np.sqrt(np.sum(((freqs - mean_f_c3) ** 2) * power_c3) / np.sum(power_c3))\n",
        "                bw_c4 = np.sqrt(np.sum(((freqs - mean_f_c4) ** 2) * power_c4) / np.sum(power_c4))\n",
        "                beta_c3 = np.sum(power_c3[(freqs >= 13) & (freqs <= 30)]) / len(c3)\n",
        "                beta_c4 = np.sum(power_c4[(freqs >= 13) & (freqs <= 30)]) / len(c4)\n",
        "\n",
        "                row.update({\n",
        "                    \"peak_freq_C3\":   float(freqs[np.argmax(power_c3)]),\n",
        "                    \"peak_freq_C4\":   float(freqs[np.argmax(power_c4)]),\n",
        "                    \"mean_freq_C3\":   float(mean_f_c3),\n",
        "                    \"mean_freq_C4\":   float(mean_f_c4),\n",
        "                    \"bw_C3\":          float(bw_c3),\n",
        "                    \"bw_C4\":          float(bw_c4),\n",
        "                    \"beta_power_C3\":  float(beta_c3),\n",
        "                    \"beta_power_C4\":  float(beta_c4),\n",
        "                    \"psd_mean_C3\":    float(Pxx.mean()),\n",
        "                    \"psd_mean_C4\":    float(Pyy.mean())\n",
        "                })\n",
        "\n",
        "                # ------------------------------------------------------------------\n",
        "                # 9) Filter bank CSP‑like band powers\n",
        "                # ------------------------------------------------------------------\n",
        "                for lo, hi in fbcsp_bands:\n",
        "                    mask = (freqs >= lo) & (freqs < hi)\n",
        "                    row[f\"fbcsp_{lo}_{hi}_C3\"] = float(np.sum(power_c3[mask]) / len(c3))\n",
        "                    row[f\"fbcsp_{lo}_{hi}_C4\"] = float(np.sum(power_c4[mask]) / len(c4))\n",
        "\n",
        "                # Accumulate\n",
        "                rows.append(row)\n",
        "                X_all.append(np.stack([c3, cz, c4], axis=1))\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # 10) Common Spatial Patterns (CSP) on concatenated trials\n",
        "    # ----------------------------------------------------------------------\n",
        "    if X_all:\n",
        "        X_all_arr = np.array(X_all)\n",
        "        cov = np.mean([np.cov(trial.T) for trial in X_all_arr], axis=0)\n",
        "        eigvals, eigvecs = np.linalg.eigh(cov)\n",
        "        n_filt = min(4, eigvecs.shape[1])\n",
        "        W = eigvecs[:, eigvals.argsort()[::-1][:n_filt]]\n",
        "\n",
        "        for row, trial in zip(rows, X_all_arr):\n",
        "            Z = trial @ W                         # spatially‑filtered signals\n",
        "            var = np.var(Z, axis=0)\n",
        "            logvar = np.log(var / var.sum())      # normalized log‑variance\n",
        "            for k, v in enumerate(logvar, start=1):\n",
        "                row[f\"csp{k}\"] = float(v)\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # 11) Save to CSV\n",
        "    # ----------------------------------------------------------------------\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEAVmni14cZT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "the_data_set=[]\n",
        "subjects_of_data = os.listdir(Path_of_the_data_set)\n",
        "the_data_set = preprocessing_and_loading(subjects_of_data,Path_of_the_data_set,\"MI_features_train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM91fBZy2Fw0"
      },
      "source": [
        "# **Inference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXX5SkeI58Ua"
      },
      "source": [
        "\n",
        "\n",
        "*  load the model , scalar and the features of the model\n",
        "*   scale all the the data\n",
        "*   select the features from the scaled data\n",
        "*   predict the data using the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NzIHoGDx1zvk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 1:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.59192999 0.40807001]\n",
            "\n",
            "Sample 2:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.62308129 0.37691871]\n",
            "\n",
            "Sample 3:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.38651235 0.61348765]\n",
            "\n",
            "Sample 4:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.52989585 0.47010415]\n",
            "\n",
            "Sample 5:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.32692386 0.67307614]\n",
            "\n",
            "Sample 6:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.68654684 0.31345317]\n",
            "\n",
            "Sample 7:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.44317995 0.55682005]\n",
            "\n",
            "Sample 8:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.42204781 0.57795219]\n",
            "\n",
            "Sample 9:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.37263424 0.62736576]\n",
            "\n",
            "Sample 10:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.50240931 0.49759069]\n",
            "\n",
            "Sample 11:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.56107491 0.43892508]\n",
            "\n",
            "Sample 12:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.38512666 0.61487334]\n",
            "\n",
            "Sample 13:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.63396905 0.36603094]\n",
            "\n",
            "Sample 14:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.55108617 0.44891384]\n",
            "\n",
            "Sample 15:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.50947281 0.4905272 ]\n",
            "\n",
            "Sample 16:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.67518949 0.32481051]\n",
            "\n",
            "Sample 17:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.45152175 0.54847825]\n",
            "\n",
            "Sample 18:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.39968162 0.60031838]\n",
            "\n",
            "Sample 19:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.43133487 0.56866513]\n",
            "\n",
            "Sample 20:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.39562361 0.60437639]\n",
            "\n",
            "Sample 21:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.54734025 0.45265974]\n",
            "\n",
            "Sample 22:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.49973514 0.50026486]\n",
            "\n",
            "Sample 23:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.61950223 0.38049777]\n",
            "\n",
            "Sample 24:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.37552083 0.62447917]\n",
            "\n",
            "Sample 25:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.40348043 0.59651957]\n",
            "\n",
            "Sample 26:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.53264419 0.46735582]\n",
            "\n",
            "Sample 27:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.40260034 0.59739966]\n",
            "\n",
            "Sample 28:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.59097158 0.40902842]\n",
            "\n",
            "Sample 29:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.51944133 0.48055867]\n",
            "\n",
            "Sample 30:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.59696908 0.40303091]\n",
            "\n",
            "Sample 31:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.49313732 0.50686268]\n",
            "\n",
            "Sample 32:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.38610209 0.61389791]\n",
            "\n",
            "Sample 33:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.61929099 0.38070901]\n",
            "\n",
            "Sample 34:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.54508214 0.45491787]\n",
            "\n",
            "Sample 35:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.61094068 0.38905932]\n",
            "\n",
            "Sample 36:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.70270375 0.29729625]\n",
            "\n",
            "Sample 37:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.65899203 0.34100798]\n",
            "\n",
            "Sample 38:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.52492127 0.47507873]\n",
            "\n",
            "Sample 39:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.45375267 0.54624733]\n",
            "\n",
            "Sample 40:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.38205751 0.61794249]\n",
            "\n",
            "Sample 41:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.40739625 0.59260375]\n",
            "\n",
            "Sample 42:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.62059658 0.37940342]\n",
            "\n",
            "Sample 43:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.40656375 0.59343625]\n",
            "\n",
            "Sample 44:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.49218342 0.50781659]\n",
            "\n",
            "Sample 45:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.62177095 0.37822906]\n",
            "\n",
            "Sample 46:\n",
            "  Predicted class: 1 (Right)\n",
            "  Class probabilities: [0.44516983 0.55483017]\n",
            "\n",
            "Sample 47:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.62763007 0.37236993]\n",
            "\n",
            "Sample 48:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.64545878 0.35454122]\n",
            "\n",
            "Sample 49:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.64172442 0.35827557]\n",
            "\n",
            "Sample 50:\n",
            "  Predicted class: 0 (Left)\n",
            "  Class probabilities: [0.5543958 0.4456042]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator VotingClassifier from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SVC was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\Badr\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SVC was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "#full feature list\n",
        "feature_names = ['min_val_C3', 'max_val_C3', 'mean_val_C3', 'energy_C3', 'min_val_C4', 'max_val_C4', 'mean_val_C4',\n",
        "                 'energy_C4', 'alpha_C3', 'alpha_C4', 'erd_C3_pct', 'erd_C4_pct', 'coh_C3C4_mu', 'imcoh_C3C4_mu',\n",
        "                 'plv_C3C4', 'range_C3', 'std_C3', 'skew_C3', 'kurt_C3', 'range_C4', 'std_C4', 'skew_C4', 'kurt_C4',\n",
        "                 'peak_freq_C3', 'peak_freq_C4', 'mean_freq_C3', 'mean_freq_C4', 'bw_C3', 'bw_C4', 'beta_power_C3',\n",
        "                 'beta_power_C4', 'psd_mean_C3', 'psd_mean_C4', 'fbcsp_8_12_C3', 'fbcsp_8_12_C4', 'fbcsp_12_16_C3',\n",
        "                 'fbcsp_12_16_C4', 'fbcsp_16_20_C3', 'fbcsp_16_20_C4', 'fbcsp_20_24_C3', 'fbcsp_20_24_C4',\n",
        "                 'fbcsp_24_28_C3', 'fbcsp_24_28_C4', 'fbcsp_28_32_C3', 'fbcsp_28_32_C4', 'csp1', 'csp2', 'csp3']\n",
        "\n",
        "# Inverse label mapping\n",
        "inverse_label_map = {0: 'Left', 1: 'Right'}\n",
        "\n",
        "# Load components\n",
        "model = joblib.load(\"best_model.pkl\")\n",
        "scaler = joblib.load(\"scaler_for_best_features2.pkl\")\n",
        "best_features = joblib.load(\"selected_feature_list2.pkl\")\n",
        "\n",
        "# Step 1: Scale full Data (must have all 48 columns, in same order)\n",
        "X_scaled_full = scaler.transform(the_data_set[feature_names])\n",
        "\n",
        "# Step 2: Wrap scaled data back into a DataFrame for column access\n",
        "X_scaled_df = pd.DataFrame(X_scaled_full, columns=feature_names)\n",
        "\n",
        "# Step 3: Select only best features (now scaled)\n",
        "X_new_scaled = X_scaled_df[best_features]\n",
        "# Step 4: Predict\n",
        "y_pred = model.predict(X_new_scaled)\n",
        "y_proba = model.predict_proba(X_new_scaled)\n",
        "\n",
        "\n",
        "\n",
        "# Step 5: Decode prediction\n",
        "decoded_preds = [inverse_label_map[p] for p in y_pred]\n",
        "# Step 6: Show results\n",
        "for i in range(len(y_pred)):\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Predicted class: {y_pred[i]} ({decoded_preds[i]})\")\n",
        "    print(f\"  Class probabilities: {y_proba[i]}\")\n",
        "    print()\n",
        "results_df = pd.DataFrame({\n",
        "\n",
        "    \"label\": decoded_preds,\n",
        "\n",
        "})\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
